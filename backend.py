# backend_sealion.py
"""
NutriHealth - Hackathon Edition
Fully dynamic nutrition chatbot using SEA-LION (via Ollama).

- NO hardcoded meal ideas
- NO static suggestions
- ALL responses generated by the model
- STRICT JSON output for React compatibility
- Follows hackathon rules: science-based, personalized, debunk myths
"""

import os
import json
import requests
from typing import Any, Dict, List, Optional
from flask import Flask, request, jsonify
from flask_cors import CORS


# === OLLAMA CONFIG ==============================================
# Every network parameter is centralized here so deployments can override
# behavior through environment variables without touching the code.

OLLAMA_CHAT_URL = os.getenv(
    "OLLAMA_CHAT_URL",
    "http://localhost:11434/v1/chat/completions",
)

SEALION_MODEL = os.getenv(
    "SEALION_MODEL",
    "aisingapore/Gemma-SEA-LION-v3-9B-IT:q4_k_m"
)


# === FLASK APP ==================================================
# Single Flask instance with CORS enabled to allow the Vite dev server
# to call these endpoints during development.

app = Flask(__name__)
CORS(app)


# === SYSTEM PROMPT (Hackathon-Required Behaviour) ===============
# The hackathon requires strict behavior. A helper keeps the prompt
# construction consistent wherever it is needed.

def build_system_prompt(topic: Optional[str]) -> str:
    """
    Build the guardrails SEA-LION must follow.
    No nutrition facts are hard-coded to comply with the hackathon brief.
    """

    base = f"""
You are **NutriHealth**, a nutrition-focused chatbot built for a hackathon.

Your required behaviour:
1. Provide **clear, evidence-based** nutrition guidance.
2. **Debunk misinformation** and explain hidden science simply.
3. Give **personalized responses** based on user input.
4. Promote **healthy, sustainable habits**.
5. Speak in a **friendly, encouraging tone**.
6. Output **STRICT JSON only** - no markdown, no backticks, no extra text.

JSON FORMAT (MANDATORY):
{{
  "reply": "string - chatbot's main answer",
  "topic": "healthy | protein | fat_loss | general",
  "suggestedQuestions": ["q1", "q2", "q3"]
}}

Avoid medical claims. If user mentions medical conditions, tell them to consult a professional.

Conversation focus: {topic or "general"}.
    """

    return base.strip()


# === OLLAMA CALL ================================================
# Wrap every model call so logging, payload tweaks, and future auth
# headers stay in one place.

def call_sealion(messages: List[Dict[str, str]]) -> str:
    payload = {
        "model": SEALION_MODEL,
        "messages": messages,
        "temperature": 0.5,
        "max_tokens": 250
    }

    response = requests.post(OLLAMA_CHAT_URL, json=payload)
    response.raise_for_status()

    return response.json()["choices"][0]["message"]["content"]


# === FALLBACK JSON PARSER ========================================
# SEA-LION is instructed to return JSON, but models occasionally wrap
# payloads with ``` fences. The helper below keeps the response usable.

def _strip_code_fences(raw: str) -> str:
    text = raw.strip()
    if text.startswith("```"):
        lines = text.splitlines()
        if lines:
            lines = lines[1:]
        if lines and lines[-1].strip().startswith("```"):
            lines = lines[:-1]
        text = "\n".join(lines).strip()
    return text


def parse_json(raw: str, fallback_topic: Optional[str]) -> Dict[str, Any]:
    cleaned = _strip_code_fences(raw)
    try:
        parsed = json.loads(cleaned)
        if not isinstance(parsed, dict):
            raise ValueError("Invalid JSON")
        parsed.setdefault("topic", fallback_topic or "general")
        parsed.setdefault("suggestedQuestions", [])
        return parsed
    except Exception:
        return {
            "reply": cleaned,
            "topic": fallback_topic or "general",
            "suggestedQuestions": []
        }


# === API ENDPOINT ===============================================
# /api/chat handles user prompts. /api/health is a lightweight probe.

@app.route("/api/chat", methods=["POST"])
def chat():
    # Pull the payload safely and sanitize obvious edge cases.
    data = request.get_json(force=True) or {}
    message = str(data.get("message", "")).strip()
    topic = data.get("topic")

    if not message:
        return jsonify({"error": "Message is required"}), 400

    system_prompt = build_system_prompt(topic)

    # SEA-LION expects an OpenAI-style conversation array.
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": message}
    ]

    try:
        raw = call_sealion(messages)
        parsed = parse_json(raw, topic)
        return jsonify(parsed), 200
    except Exception:
        # On failure we avoid surfacing internals to the client.
        return jsonify({
            "reply": "The nutrition engine is unavailable. Please try again later.",
            "topic": topic or "general",
            "suggestedQuestions": []
        }), 500


@app.route("/api/health", methods=["GET"])
def health():
    # Basic readiness endpoint for render hosts and manual checks.
    return jsonify({
        "status": "ok",
        "model": SEALION_MODEL,
        "service": "NutriHealth Hackathon Backend"
    })


if __name__ == "__main__":
    # Development server only; production should use a WSGI container.
    app.run(host="0.0.0.0", port=5000, debug=True)
